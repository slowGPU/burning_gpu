{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPRECATED\n",
    "use `dataset/grappa.py` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.external.NAFNet.NAFNet_arch import NAFNet\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "\n",
    "from common.utils import save_reconstructions\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import fastmri\n",
    "import h5py\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional, Tuple, List, Union, Final\n",
    "\n",
    "PUBLIC_ACCS = [\"acc4\", \"acc5\", \"acc8\"]\n",
    "root_path = \"/home/Data/leaderboard\"\n",
    "out_path = \"reconstructions/grappa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     reconstructions[fname] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([reconstructions[fname][\u001b[38;5;28mslice\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(reconstructions[fname])])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;129;01min\u001b[39;00m PUBLIC_ACCS:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43msave_reconstructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/public\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     save_reconstructions(reconstructions, Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/private\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/burning_gpu/common/utils.py:30\u001b[0m, in \u001b[0;36msave_reconstructions\u001b[0;34m(reconstructions, out_dir, targets, inputs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname, recons \u001b[38;5;129;01min\u001b[39;00m reconstructions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(out_dir \u001b[38;5;241m/\u001b[39m fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 30\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreconstruction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m             f\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mtargets[fname])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/dataset.py:166\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, name, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl, dapl\u001b[38;5;241m=\u001b[39mdapl)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 166\u001b[0m     \u001b[43mdset_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for acc in os.listdir(root_path):\n",
    "    reconstructions = defaultdict(dict)\n",
    "    file_list = os.listdir(f\"{root_path}/{acc}/image\")\n",
    "    for file in file_list:\n",
    "        grappa = h5py.File(f\"{root_path}/{acc}/image/{file}\", \"r\")[\"image_grappa\"]\n",
    "        for i in range(grappa.shape[0]):\n",
    "            reconstructions[file][i] = np.array(grappa[i])\n",
    "    for fname in reconstructions:\n",
    "        reconstructions[fname] = np.stack([reconstructions[fname][slice] for slice in sorted(reconstructions[fname])])\n",
    "\n",
    "    if acc in PUBLIC_ACCS:\n",
    "        save_reconstructions(reconstructions, Path(f\"{out_path}/public\"))\n",
    "\n",
    "    else:\n",
    "        save_reconstructions(reconstructions, Path(f\"{out_path}/private\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grappa_DataType = Tuple[\n",
    "    torch.Tensor, # Grappa Image\n",
    "    Optional[torch.Tensor], # Ground Truth Image (target)\n",
    "]\n",
    "\n",
    "class GrappaDataset(Dataset):\n",
    "    def __init__(self, root_path: str, input_key: str, target_key: Optional[str] = None) -> None:\n",
    "        self.root_path = root_path\n",
    "        self.input_key = input_key\n",
    "        self.target_key = target_key\n",
    "        \n",
    "        self.file_list = os.listdir(f\"{root_path}/image\")\n",
    "        self.grappa_data = defaultdict(dict)\n",
    "        self.target_data = defaultdict(dict)\n",
    "        \n",
    "        for file in self.file_list:\n",
    "            grappa = h5py.File(f\"{root_path}/image/{file}\", \"r\")[input_key]\n",
    "            for i in range(grappa.shape[0]):\n",
    "                self.grappa_data[file][i] = torch.tensor(grappa[i])\n",
    "\n",
    "        if target_key is not None:\n",
    "            for file in self.file_list:\n",
    "                target = h5py.File(f\"{root_path}/image/{file}\", \"r\")[target_key]\n",
    "                for i in range(target.shape[0]):\n",
    "                    self.target_data[file][i] = torch.tensor(target[i])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Grappa_DataType:\n",
    "        if self.target_key is not None:\n",
    "            return self.grappa_data[self.file_list[idx]], self.target_data[self.file_list[idx]]\n",
    "        return self.grappa_data[self.file_list[idx]], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grappa_dataloaders = {}\n",
    "# for acc in os.listdir(root_path):\n",
    "#     grappa_dataset = GrappaDataset(root_path, acc)\n",
    "#     grappa_raw_data = h5py.File(f\"{root_path}/{acc}/image/{file}\", \"r\")[\"image_grappa\"]\n",
    "#     grappa_data = torch.tensor(grappa_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrappaDataModule(L.LightningDataModule):\n",
    "    PUBLIC_ACCS: Final[List[str]] = [\"acc4\", \"acc5\", \"acc8\"]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[str, Path],\n",
    "            batch_size: int = 1,\n",
    "            input_key: str = \"image_grappa\",\n",
    "            target_key: str = \"image_label\",\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.root = Path(root)\n",
    "        self.batch_size = batch_size\n",
    "        self.input_key = input_key\n",
    "        self.target_key = target_key\n",
    "        \n",
    "        public_acc, private_acc = sorted(\n",
    "            os.listdir(root / \"leaderboard\"),\n",
    "            key=lambda x: (x not in self.PUBLIC_ACCS),\n",
    "        )\n",
    "\n",
    "        self.path_train = self.root / \"train\"\n",
    "        self.path_val = self.root / \"val\"\n",
    "        self.path_test = self.root / \"leaderboard\" / public_acc\n",
    "        self.path_predict = self.root / \"leaderboard\" / private_acc\n",
    "\n",
    "        self.data_train: Optional[GrappaDataset] = None\n",
    "        self.data_val: Optional[GrappaDataset] = None\n",
    "        self.data_test: Optional[GrappaDataset] = None\n",
    "        self.data_predict: Optional[GrappaDataset] = None\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage in (\"fit\", \"train\") and self.dataset_train is None:\n",
    "            self.dataset_train = GrappaDataset(self.path_train, self.input_key, self.target_key)\n",
    "\n",
    "        if stage in (\"fit\", \"val\") and self.dataset_val is None:\n",
    "            self.dataset_val = GrappaDataset(self.path_val, self.input_key, self.target_key)\n",
    "\n",
    "        if stage in (\"test\",) and self.dataset_test is None:\n",
    "            self.dataset_test = GrappaDataset(self.path_test, self.input_key)\n",
    "\n",
    "        if stage in (\"predict\",) and self.dataset_predict is None:\n",
    "            self.dataset_predict = GrappaDataset(self.path_predict, self.input_key)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset_val, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset_predict, batch_size=self.batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NAFNet(\n",
    "    img_channel=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAFNet(\n",
       "  (intro): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ending): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoders): ModuleList()\n",
       "  (decoders): ModuleList()\n",
       "  (middle_blks): Sequential(\n",
       "    (0): NAFBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sca): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (sg): SimpleGate()\n",
       "      (conv4): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm1): LayerNorm2d()\n",
       "      (norm2): LayerNorm2d()\n",
       "      (dropout1): Identity()\n",
       "      (dropout2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList()\n",
       "  (downs): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NAFNet(\n",
    "    img_channel=2,\n",
    "    width=64,\n",
    "    enc_blk_nums=[1, 1, 1, 28],\n",
    "    middle_blk_num=4,\n",
    "    dec_blk_nums=[1, 1, 1, 1],\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAFNet(\n",
       "  (intro): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (ending): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoders): ModuleList()\n",
       "  (decoders): ModuleList()\n",
       "  (middle_blks): Sequential(\n",
       "    (0): NAFBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sca): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (sg): SimpleGate()\n",
       "      (conv4): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm1): LayerNorm2d()\n",
       "      (norm2): LayerNorm2d()\n",
       "      (dropout1): Identity()\n",
       "      (dropout2): Identity()\n",
       "    )\n",
       "    (1): NAFBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sca): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (sg): SimpleGate()\n",
       "      (conv4): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm1): LayerNorm2d()\n",
       "      (norm2): LayerNorm2d()\n",
       "      (dropout1): Identity()\n",
       "      (dropout2): Identity()\n",
       "    )\n",
       "    (2): NAFBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sca): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (sg): SimpleGate()\n",
       "      (conv4): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm1): LayerNorm2d()\n",
       "      (norm2): LayerNorm2d()\n",
       "      (dropout1): Identity()\n",
       "      (dropout2): Identity()\n",
       "    )\n",
       "    (3): NAFBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sca): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (sg): SimpleGate()\n",
       "      (conv4): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm1): LayerNorm2d()\n",
       "      (norm2): LayerNorm2d()\n",
       "      (dropout1): Identity()\n",
       "      (dropout2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList()\n",
       "  (downs): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
